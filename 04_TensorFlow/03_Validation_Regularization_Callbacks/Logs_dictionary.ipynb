{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the logs dictionary\n",
    "\n",
    "In this notebook, we will learn how to take advantage of the `logs` dictionary in Keras to define our own callbacks and check the progress of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `logs` dictionary stores the loss value, along with all of the metrics we are using at the end of a batch or epoch.\n",
    "\n",
    "We can incorporate information from the `logs` dictionary into our own custom callbacks.\n",
    "\n",
    "Let's see this in action in the context of a model we will construct and fit to the `sklearn` diabetes dataset that we have been using in this module.\n",
    "\n",
    "Let's first import the dataset, and split it into the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes_dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = diabetes_dataset['data']\n",
    "targets = diabetes_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compile the model, with\n",
    "* Mean squared error as the loss function,\n",
    "* the Adam optimizer, and \n",
    "* Mean absolute error (`mae`) as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a custom callback\n",
    "\n",
    "Now we define our custom callback using the `logs` dictionary to access the loss and metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the custom callback\n",
    "\n",
    "class LossAndMetricCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    # Print the loss after every second batch in the training set\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch %2 ==0:\n",
    "            print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n",
    "    \n",
    "    # Print the loss after each batch in the test set\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n",
    "\n",
    "    # Print the loss and mean absolute error after each epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Epoch {}: Average loss is {:7.2f}, mean absolute error is {:7.2f}.'.format(epoch, logs['loss'], logs['mae']))\n",
    "    \n",
    "    # Notify the user when prediction has finished on each batch\n",
    "    def on_predict_batch_end(self,batch, logs=None):\n",
    "        print(\"Finished prediction on batch {}!\".format(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the model to the data, and specify that we would like to use our custom callback `LossAndMetricCallback()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After batch 0, the loss is 29358.86.\n",
      "\n",
      " After batch 2, the loss is 31313.65.\n",
      "Epoch 0: Average loss is 29011.41, mean absolute error is  151.78.\n",
      "\n",
      " After batch 0, the loss is 27266.15.\n",
      "\n",
      " After batch 2, the loss is 28077.89.\n",
      "Epoch 1: Average loss is 28869.78, mean absolute error is  151.36.\n",
      "\n",
      " After batch 0, the loss is 31728.62.\n",
      "\n",
      " After batch 2, the loss is 26891.61.\n",
      "Epoch 2: Average loss is 28672.13, mean absolute error is  150.76.\n",
      "\n",
      " After batch 0, the loss is 25593.59.\n",
      "\n",
      " After batch 2, the loss is 29093.06.\n",
      "Epoch 3: Average loss is 28380.28, mean absolute error is  149.89.\n",
      "\n",
      " After batch 0, the loss is 29276.32.\n",
      "\n",
      " After batch 2, the loss is 31845.50.\n",
      "Epoch 4: Average loss is 27995.71, mean absolute error is  148.70.\n",
      "\n",
      " After batch 0, the loss is 30280.61.\n",
      "\n",
      " After batch 2, the loss is 28386.69.\n",
      "Epoch 5: Average loss is 27454.66, mean absolute error is  147.03.\n",
      "\n",
      " After batch 0, the loss is 25469.94.\n",
      "\n",
      " After batch 2, the loss is 29545.54.\n",
      "Epoch 6: Average loss is 26711.72, mean absolute error is  144.72.\n",
      "\n",
      " After batch 0, the loss is 22620.26.\n",
      "\n",
      " After batch 2, the loss is 22447.23.\n",
      "Epoch 7: Average loss is 25779.95, mean absolute error is  141.76.\n",
      "\n",
      " After batch 0, the loss is 25984.47.\n",
      "\n",
      " After batch 2, the loss is 27239.82.\n",
      "Epoch 8: Average loss is 24609.34, mean absolute error is  137.90.\n",
      "\n",
      " After batch 0, the loss is 22130.18.\n",
      "\n",
      " After batch 2, the loss is 17734.25.\n",
      "Epoch 9: Average loss is 23209.15, mean absolute error is  133.11.\n",
      "\n",
      " After batch 0, the loss is 19024.15.\n",
      "\n",
      " After batch 2, the loss is 21484.43.\n",
      "Epoch 10: Average loss is 21529.50, mean absolute error is  127.14.\n",
      "\n",
      " After batch 0, the loss is 18753.16.\n",
      "\n",
      " After batch 2, the loss is 15363.20.\n",
      "Epoch 11: Average loss is 19651.72, mean absolute error is  120.15.\n",
      "\n",
      " After batch 0, the loss is 20797.30.\n",
      "\n",
      " After batch 2, the loss is 15253.34.\n",
      "Epoch 12: Average loss is 17623.22, mean absolute error is  111.85.\n",
      "\n",
      " After batch 0, the loss is 15879.84.\n",
      "\n",
      " After batch 2, the loss is 16476.27.\n",
      "Epoch 13: Average loss is 15384.61, mean absolute error is  102.50.\n",
      "\n",
      " After batch 0, the loss is 18021.78.\n",
      "\n",
      " After batch 2, the loss is 13294.63.\n",
      "Epoch 14: Average loss is 13281.16, mean absolute error is   93.07.\n",
      "\n",
      " After batch 0, the loss is 13457.40.\n",
      "\n",
      " After batch 2, the loss is 13090.27.\n",
      "Epoch 15: Average loss is 11254.23, mean absolute error is   84.16.\n",
      "\n",
      " After batch 0, the loss is 11886.29.\n",
      "\n",
      " After batch 2, the loss is 9469.66.\n",
      "Epoch 16: Average loss is 9305.91, mean absolute error is   75.54.\n",
      "\n",
      " After batch 0, the loss is 7758.45.\n",
      "\n",
      " After batch 2, the loss is 7308.52.\n",
      "Epoch 17: Average loss is 7848.77, mean absolute error is   68.99.\n",
      "\n",
      " After batch 0, the loss is 6421.07.\n",
      "\n",
      " After batch 2, the loss is 5667.95.\n",
      "Epoch 18: Average loss is 6925.83, mean absolute error is   64.19.\n",
      "\n",
      " After batch 0, the loss is 6975.90.\n",
      "\n",
      " After batch 2, the loss is 5676.10.\n",
      "Epoch 19: Average loss is 6225.53, mean absolute error is   61.44.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=20, batch_size=100, callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use our callback in the `evaluate` function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After batch 0, the loss is 24125.98.\n",
      "\n",
      " After batch 1, the loss is 15744.18.\n",
      "\n",
      " After batch 2, the loss is 19031.77.\n",
      "\n",
      " After batch 3, the loss is 20733.69.\n",
      "\n",
      " After batch 4, the loss is 7094.27.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "model_eval = model.evaluate(test_data, test_targets, batch_size=10, \n",
    "                            callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And also the `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished prediction on batch 0!\n",
      "Finished prediction on batch 1!\n",
      "Finished prediction on batch 2!\n",
      "Finished prediction on batch 3!\n",
      "Finished prediction on batch 4!\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the model\n",
    "\n",
    "model_pred = model.predict(test_data, batch_size=10,\n",
    "                           callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application - learning rate scheduler\n",
    "Let's now look at a more sophisticated custom callback. \n",
    "\n",
    "We are going to define a callback to change the learning rate of the optimiser of a model during training. We will do this by specifying the epochs and new learning rates where we would like it to be changed.\n",
    "\n",
    "First we define the auxillary function that returns the learning rate for each epoch based on our schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate schedule. The tuples below are (start_epoch, new_learning_rate)\n",
    "\n",
    "lr_schedule = [\n",
    "    (4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)\n",
    "]\n",
    "\n",
    "def get_new_epoch_lr(epoch, lr):\n",
    "    # Checks to see if the input epoch is listed in the learning rate schedule \n",
    "    # and if so, returns index in lr_schedule\n",
    "    epoch_in_sched = [i for i in range(len(lr_schedule)) if lr_schedule[i][0] == int(epoch)]\n",
    "    if len(epoch_in_sched) > 0:\n",
    "        # If it is, return the learning rate corresponding to the epoch\n",
    "        return lr_schedule[epoch_in_sched[0]][1]\n",
    "        # Otherwise, return the existing learning rate\n",
    "    else:\n",
    "        return lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the callback itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom callback\n",
    "\n",
    "class LRScheduler(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, new_lr):\n",
    "        super(LRScheduler, self).__init__()\n",
    "        # Add the new learning rate function to our callback\n",
    "        self.new_lr = new_lr\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Make sure that the optimizer we have chosen has a learning rate, and raise an error if not\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Error: Optimizer does not has a learning rate.')\n",
    "            \n",
    "        # Get the current learning rate\n",
    "        curr_rate = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "        \n",
    "        # Call th auxillary function to get the scheduled learning rate for the current epoch\n",
    "        scheduled_rate = self.new_lr(epoch, curr_rate)\n",
    "        \n",
    "        # Set the learning rate to the scheduled learning rate\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n",
    "        print(f'Learning rate for epoch {epoch} is {scheduled_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the same model as before\n",
    "\n",
    "new_model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "new_model.compile(loss='mse',\n",
    "                optimizer=\"adam\",\n",
    "                metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate for epoch 0 is 0.0010000000474974513\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\n",
      "Learning rate for epoch 2 is 0.0010000000474974513\n",
      "Learning rate for epoch 3 is 0.0010000000474974513\n",
      "Learning rate for epoch 4 is 0.03\n",
      "Learning rate for epoch 5 is 0.029999999329447746\n",
      "Learning rate for epoch 6 is 0.029999999329447746\n",
      "Learning rate for epoch 7 is 0.02\n",
      "Learning rate for epoch 8 is 0.019999999552965164\n",
      "Learning rate for epoch 9 is 0.019999999552965164\n",
      "Learning rate for epoch 10 is 0.019999999552965164\n",
      "Learning rate for epoch 11 is 0.005\n",
      "Learning rate for epoch 12 is 0.004999999888241291\n",
      "Learning rate for epoch 13 is 0.004999999888241291\n",
      "Learning rate for epoch 14 is 0.004999999888241291\n",
      "Learning rate for epoch 15 is 0.007\n",
      "Learning rate for epoch 16 is 0.007000000216066837\n",
      "Learning rate for epoch 17 is 0.007000000216066837\n",
      "Learning rate for epoch 18 is 0.007000000216066837\n",
      "Learning rate for epoch 19 is 0.007000000216066837\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with our learning rate scheduler callback\n",
    "\n",
    "new_history = new_model.fit(train_data, train_targets, epochs=20,\n",
    "                            batch_size=100, callbacks=[LRScheduler(get_new_epoch_lr)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading and resources\n",
    "* https://www.tensorflow.org/guide/keras/custom_callback\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
